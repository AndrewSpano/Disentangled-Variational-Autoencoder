{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAE.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1yOhjm82JzRCzEfkCHB7FCbdM3NZJ3eTE","authorship_tag":"ABX9TyN7s0FM5EpkLlkco8H9MKRq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1b31160c33c440f1831806645e9495ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c52d6458466348a68bfb83af4b7b8361","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2822a27fb2444a6fb12abfb1a338b4f9","IPY_MODEL_6f168c746ccc41d48cdba0f24b5e2908"]}},"c52d6458466348a68bfb83af4b7b8361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"2822a27fb2444a6fb12abfb1a338b4f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_72a82e48b1f448279fba4e145d44aa9d","_dom_classes":[],"description":"Epoch 4: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1563,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1563,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f6192dbffcd947d38e3e08d7ec18932a"}},"6f168c746ccc41d48cdba0f24b5e2908":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8bb8159bab374924b30cebe36072b74b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1563/1563 [05:07&lt;00:00,  5.08it/s, loss=1961.608, v_num=2]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e9109624cd55444da0b8fb57173ec8a7"}},"72a82e48b1f448279fba4e145d44aa9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f6192dbffcd947d38e3e08d7ec18932a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bb8159bab374924b30cebe36072b74b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e9109624cd55444da0b8fb57173ec8a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e32f32f3b8bf496889bab0f0fb087d21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5146752425ac46158bc3d467e85ad2cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_66e666f8c7de40d8bfcdec9b14a81337","IPY_MODEL_a6c5c9b12937462880ae71927c1dc701"]}},"5146752425ac46158bc3d467e85ad2cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"66e666f8c7de40d8bfcdec9b14a81337":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e7c61aee2f404a548f89755ff505228d","_dom_classes":[],"description":"Testing: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c3684f7e65f34974b31aff40176f9fb6"}},"a6c5c9b12937462880ae71927c1dc701":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_765bb6e9f9f4465fa750156128871652","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 313/313 [00:11&lt;00:00, 27.59it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc27a6489b8b49d58581893441bff3d6"}},"e7c61aee2f404a548f89755ff505228d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c3684f7e65f34974b31aff40176f9fb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"765bb6e9f9f4465fa750156128871652":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dc27a6489b8b49d58581893441bff3d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"SH8CShRYPYhv"},"source":["#Installations & Imports"]},{"cell_type":"code","metadata":{"id":"LBLVaAuonBIw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607108226546,"user_tz":-120,"elapsed":3664,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}},"outputId":"f132a6bc-3c43-46e7-affb-f8e697f82ad0"},"source":["!pip install pytorch-lightning"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (1.0.8)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.7.0+cu101)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n","Requirement already satisfied: fsspec>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.8.4)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.18.5)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.3.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (3.7.4.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.33.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.7.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.35.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (50.3.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.17.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (2.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lf1HdB31O5oq","executionInfo":{"status":"ok","timestamp":1607108231710,"user_tz":-120,"elapsed":8813,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["import torch\n","import torchvision\n","import pytorch_lightning as pl\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMBJm1iWV2m9"},"source":["#Parameter Initialization"]},{"cell_type":"code","metadata":{"id":"Qq6avijbmPYw","executionInfo":{"status":"ok","timestamp":1607108231710,"user_tz":-120,"elapsed":8808,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["import os\n","# Insert your own path here\n","ds_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Machine Learning\", \"Datasets\")\n","\n","variation = \"VAE\"\n","\n","configuration = {\n","    \"dataset\": \"CIFAR10\",\n","    \"path\": ds_path\n","}\n","\n","architecture = {\n","    \"conv_layers\": 3,\n","    \"conv_channels\": [64, 128, 512],\n","    \"conv_kernel_sizes\": [(9, 9), (7, 7), (5, 5)],\n","    \"conv_strides\": [(1, 1), (1, 1), (1, 1)],\n","    \"conv_paddings\": [(1, 1), (1, 1), (1, 1)],\n","    \"z_dimension\": 256\n","}\n","\n","hyperparameters = {\n","    \"epochs\": 5,\n","    \"batch_size\": 32,\n","    \"learning_rate\": 1e-6\n","}"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_6ejOn_V-YN","executionInfo":{"status":"ok","timestamp":1607108231711,"user_tz":-120,"elapsed":8803,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["def prepare_dataset(configuration):\n","    \"\"\"\n","    :param dict configuration: The configuration dictionary returned by parse_config_file\n","\n","    :return:        A dictionary containing information about the dataset used\n","\n","    Function used to set some values used by the model based on the dataset selected\n","    \"\"\"\n","    dataset_info = {}\n","    if (configuration[\"dataset\"] == \"MNIST\"):\n","        dataset_info[\"ds_method\"] = torchvision.datasets.MNIST\n","        dataset_info[\"ds_shape\"] = (1, 28, 28)\n","        dataset_info[\"ds_path\"] = configuration[\"path\"]\n","    elif (configuration[\"dataset\"] == \"CIFAR10\"):\n","        dataset_info[\"ds_method\"] = torchvision.datasets.CIFAR10\n","        dataset_info[\"ds_shape\"] = (3, 32, 32)\n","        dataset_info[\"ds_path\"] = configuration[\"path\"]\n","    else:\n","        print(\"Currently only MNIST & CIFAR10 datasets are supported\")\n","        return None\n","\n","    return dataset_info"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-eW2P6roRBo","executionInfo":{"status":"ok","timestamp":1607108231712,"user_tz":-120,"elapsed":8799,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["dataset_info = prepare_dataset(configuration)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wcGIiNyWGre"},"source":["#Model Declaration"]},{"cell_type":"markdown","metadata":{"id":"S8n44hIDWRcm"},"source":["#####Utility Functions"]},{"cell_type":"code","metadata":{"id":"RlfcNIqBWTW4","executionInfo":{"status":"ok","timestamp":1607108231713,"user_tz":-120,"elapsed":8794,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["from torch import nn\n","def compute_output_shape(current_shape, kernel_size, stride, padding):\n","    \"\"\"\n","    :param tuple current_shape:  The current shape of the data before a convolution is applied.\n","    :param tuple kernel_size:    The kernel size of the current convolution operation.\n","    :param tuple stride:         The stride of the current convolution operation.\n","    :param tuple padding:        The padding of the current convolution operation.\n","\n","    :return:         The shape after a convolution operation with the above parameters is applied\n","                     (as a tuple).\n","                     The formula used to compute the final shape is\n","\n","        component[i] = floor((W[i] - K[i] + 2 * P[i]) / S[i]) + 1\n","\n","        where, W = input shape of the data\n","               K = kernel size\n","               P = padding\n","               S = stride\n","    \"\"\"\n","    # get the dimension of the data\n","    dimensions = len(current_shape)\n","    # compute each component using the above formula and return\n","    return tuple((current_shape[i] - kernel_size[i] + 2 * padding[i]) // stride[i] + 1\n","                 for i in range(dimensions))\n","\n","\n","def invalid_shape(current_shape):\n","    \"\"\"\n","    :param tuple current_shape: The current shape of the data after a convolution is applied.\n","\n","    :return:        True if the shape is invalid, that is, a negative or 0 components exists. Else,\n","                    it returns False.\n","    \"\"\"\n","    # check all components\n","    for component in current_shape:\n","        if component <= 0:\n","            return True\n","    # return False if they are ok\n","    return False\n","\n","\n","def create_encoder(architecture, input_shape):\n","    \"\"\"\n","    :param dict architecture:  A dictionary containing the hyperparameters that define the\n","                               architecture of the model.\n","    :param tuple input_shape:  A tuple that corresponds to the shape of the input.\n","\n","    :return:             A PyTorch Sequential model that represents the encoder part of a VAE.\n","\n","    This method builds the encoder part of a VAE and returns it. It is common for all types of VAE.\n","    \"\"\"\n","\n","    # the number of channels that the input image has\n","    in_channels = input_shape[0]\n","\n","    # keep track of the current Height and Width of the image\n","    current_shape = (input_shape[1], input_shape[2])\n","\n","    # build the encoder part\n","    sets_of_conv_selu_bn = []\n","\n","    # iterate through the lists that define the architecture of the encoder\n","    for layer in range(architecture[\"conv_layers\"]):\n","\n","        # define the number of output channels (filters) for this layer\n","        out_channels = architecture[\"conv_channels\"][layer]\n","\n","        # add a set of Convolutional - SeLU - Batch Normalization sequential layers\n","        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n","                         kernel_size=architecture[\"conv_kernel_sizes\"][layer],\n","                         stride=architecture[\"conv_strides\"][layer],\n","                         padding=architecture[\"conv_paddings\"][layer])\n","        selu = nn.LeakyReLU()\n","        batch_norm = nn.BatchNorm2d(out_channels)\n","\n","        # define a sequential model with the above architecture append it to the list\n","        sets_of_conv_selu_bn.append(nn.Sequential(conv, selu, batch_norm))\n","\n","        # compute the new shape of the image\n","        current_shape = compute_output_shape(current_shape,\n","                                             architecture[\"conv_kernel_sizes\"][layer],\n","                                             stride=architecture[\"conv_strides\"][layer],\n","                                             padding=architecture[\"conv_paddings\"][layer])\n","\n","        # make sure that the shape is valid, and if not, raise an error\n","        if invalid_shape(current_shape):\n","            print(\"Architecture is invalid, please modify it\")\n","            return None\n","\n","\n","        # the output channels of the current layer becomes the input channels of the next layer\n","        in_channels = out_channels\n","\n","    # create a Sequential model and return it (* asterisk is used to unpack the list)\n","    return nn.Sequential(*sets_of_conv_selu_bn), current_shape\n","\n","\n","def create_decoder(architecture):\n","    \"\"\"\n","    :param dict architecture:  A dictionary containing the hyperparameters that define the\n","                               architecture of the model.\n","\n","    :return:            A PyTorch Sequential model that represents the decoder part of a VAE.\n","\n","    This method builds the decoder part of a VAE and returns it. It is common for all types of VAE.\n","    \"\"\"\n","    # now start building the decoder part\n","    sets_of_convtr_selu_bn = []\n","\n","    # define the current number of channels (after the reformation of the latent vector z)\n","    in_channels = architecture[\"conv_channels\"][-1]\n","\n","    # iterate through the lists that define the architecture of the decoder\n","    for layer in range(architecture[\"conv_layers\"] - 1, -1, -1):\n","\n","        # define the number of output channels (filters) for this layer\n","        out_channels = architecture[\"conv_channels\"][layer]\n","\n","        # add a set of ConvolutionalTranspose - SeLU - Batch Normalization sequential layers\n","        convtr = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n","                                    kernel_size=architecture[\"conv_kernel_sizes\"][layer],\n","                                    stride=architecture[\"conv_strides\"][layer],\n","                                    padding=architecture[\"conv_paddings\"][layer])\n","        selu = nn.LeakyReLU()\n","        batch_norm = nn.BatchNorm2d(out_channels)\n","\n","        # define a sequential model with this architecture append it to the list\n","        sets_of_convtr_selu_bn.append(nn.Sequential(convtr, selu, batch_norm))\n","\n","        # the output channels of the current layer becomes the input channels of the next layer\n","        in_channels = out_channels\n","\n","    # create a Sequential model and return it (* asterisk is used to unpack the list)\n","    return nn.Sequential(*sets_of_convtr_selu_bn)\n","\n","\n","def create_output_layer(architecture, input_shape):\n","    \"\"\"\n","    :param dict architecture:  A dictionary containing the hyperparameters that define the\n","                               architecture of the model.\n","    :param tuple input_shape:  A tuple that corresponds to the shape of the input.\n","\n","    :return:             A PyTorch Sequential model that represents the output layer of a VAE.\n","\n","    This method creates the output layer of a VAE, that is, the layer where the data from the\n","    output of the decoder gets fed in order to be finally reconstructed.\n","    \"\"\"\n","\n","    # define the number of input channels of the last layer\n","    in_channels = architecture[\"conv_channels\"][0]\n","\n","    # define the output layer: ConvTranspose2d -> SELU -> Batch Norm -> Conv2d -> Sigmoid\n","    convtr = nn.ConvTranspose2d(in_channels=in_channels, out_channels=in_channels,\n","                                kernel_size=architecture[\"conv_kernel_sizes\"][0],\n","                                stride=architecture[\"conv_strides\"][0],\n","                                padding=architecture[\"conv_paddings\"][0])\n","    selu = nn.LeakyReLU()\n","    batch_norm = nn.BatchNorm2d(in_channels)\n","    conv = nn.Conv2d(in_channels=in_channels, out_channels=input_shape[0],\n","                     kernel_size=architecture[\"conv_kernel_sizes\"][0],\n","                     stride=architecture[\"conv_strides\"][0],\n","                     padding=architecture[\"conv_paddings\"][0])\n","    sigmoid = nn.Sigmoid()\n","\n","    # create a Sequential model and return it\n","    return nn.Sequential(convtr, selu, batch_norm, conv, sigmoid)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqv8srojWLdA"},"source":["###Variational Auto-Encoder"]},{"cell_type":"code","metadata":{"id":"OJePIeJfWKqF","executionInfo":{"status":"ok","timestamp":1607108231991,"user_tz":-120,"elapsed":9066,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["import multiprocessing\n","\n","class VAE(pl.LightningModule):\n","    \"\"\" Class that implements a Variational Autoencoder \"\"\"\n","\n","    def __init__(self, architecture, hyperparameters, dataset_info):\n","        \"\"\"\n","        :param dict architecture:      A dictionary containing the hyperparameters that define the\n","                                       architecture of the model.\n","        :param dict hyperparameters:   A tuple that corresponds to the shape of the input.\n","        :param dict dataset_info:      The dimension of the latent vector z (bottleneck).\n","\n","        :return:                       An instance of the VAE class\n","\n","        The constructor of the Variational Autoencoder.\n","        \"\"\"\n","\n","        # call the constructor of the super class\n","        super(VAE, self).__init__()\n","\n","        # initialize class variables regarding the architecture of the model\n","        self.conv_layers = architecture[\"conv_layers\"]\n","        self.conv_channels = architecture[\"conv_channels\"]\n","        self.conv_kernel_sizes = architecture[\"conv_kernel_sizes\"]\n","        self.conv_strides = architecture[\"conv_strides\"]\n","        self.conv_paddings = architecture[\"conv_paddings\"]\n","        self.z_dim = architecture[\"z_dimension\"]\n","\n","        # unpack the \"hyperparameters\" dictionary\n","        self.batch_size = hyperparameters[\"batch_size\"]\n","        self.learning_rate = hyperparameters[\"learning_rate\"]\n","        self.scheduler_step_size = hyperparameters[\"epochs\"]//4\n","\n","        # unpack the \"dataset_info\" dictionary\n","        self.dataset_method = dataset_info[\"ds_method\"]\n","        self.dataset_shape = dataset_info[\"ds_shape\"]\n","        self.dataset_path = dataset_info[\"ds_path\"]\n","\n","        # build the encoder\n","        self.encoder, self.encoder_output_shape = create_encoder(architecture, self.dataset_shape)\n","\n","        # compute the length of the output of the decoder once it has been flattened\n","        in_features = self.conv_channels[-1] * np.prod(self.encoder_output_shape[:])\n","        # now define the mean and standard deviation layers\n","        self.mean_layer = nn.Linear(in_features=in_features, out_features=self.z_dim)\n","        self.std_layer = nn.Linear(in_features=in_features, out_features=self.z_dim)\n","\n","        # use a linear layer for the input of the decoder\n","        in_channels = self.conv_channels[-1]\n","        self.decoder_input = nn.Linear(in_features=self.z_dim, out_features=in_features)\n","\n","        # build the decoder\n","        self.decoder = create_decoder(architecture)\n","\n","        # build the output layer\n","        self.output_layer = create_output_layer(architecture, self.dataset_shape)\n","\n","    def _encode(self, X):\n","        \"\"\"\n","        :param Tensor X:  Input to encode into mean and standard deviation.\n","                          (N, input_shape[1], H, W)\n","\n","        :return:          A tuple with the mean and std tensors that the encoder produces\n","                          for input X. (N, z_dim)\n","\n","        This method applies forward propagation to the self.encoder in order to get the mean and\n","        standard deviation of the latent vector z.\n","        \"\"\"\n","        # run the input through the encoder part of the Network\n","        encoded_input = self.encoder(X)\n","\n","        # flatten so that it can be fed to the mean and standard deviation layers\n","        encoded_input = torch.flatten(encoded_input, start_dim=1)\n","\n","        # compute the mean and standard deviation\n","        mean = self.mean_layer(encoded_input)\n","        std = self.std_layer(encoded_input)\n","\n","        return mean, std\n","\n","    def _compute_latent_vector(self, mean, std):\n","        \"\"\"\n","        :param Tensor mean:  The mean of the latent vector z following a Gaussian distribution.\n","                             (N, z_dim)\n","        :param Tensor std:   The standard deviation of the latent vector z following a Gaussian\n","                             distribution. (N, z_dim)\n","\n","        :return:             A Tensor of the Linear combination of the mean and standard deviation, where the latter\n","                             factor is multiplied with a random variable epsilon ~ N(0, 1). Basically\n","                             the latent vector z. (N, z_dim)\n","\n","        This method computes the latent vector z by applying the reparameterization trick to the\n","        output of the mean and standard deviation layers, in order to be able to later compute the\n","        gradient. The stochasticiy here is introduced by the factor epsilon, which is an independent\n","        node. Thus, we do not have to compute its gradient during backpropagation.\n","        \"\"\"\n","\n","        # compute the stochastic node epsilon\n","        epsilon = torch.randn_like(std)\n","        # raise the standard deviation to an exponent, to improve numberical stability\n","        std = torch.exp(1/2 * std)\n","\n","        # compute the linear combination of the above attributes and return\n","        return mean + epsilon * std\n","\n","    def _decode(self, z):\n","        \"\"\"\n","        :param Tensor z:   Latent vector computed using the mean and variance layers (with the\n","                           reparameterization trick). (N, z_dim)\n","\n","        :return:           A Tensor with The output of the decoder part of the network.\n","                           (N, input_shape[1], H, W)\n","\n","        This method performs forward propagation of the latent vector through the decoder of the\n","        VAE to get the final output of the network.\n","        \"\"\"\n","        # run the latent vector through the \"input decoder\" layer\n","        decoder_input = self.decoder_input(z)\n","\n","        # convert back the shape that will be fed to the decoder\n","        height = self.encoder_output_shape[0]\n","        width = self.encoder_output_shape[1]\n","        decoder_input = decoder_input.view(-1, self.conv_channels[-1], height, width)\n","\n","        # run through the decoder\n","        decoder_output = self.decoder(decoder_input)\n","\n","        # run through the output layer and return\n","        network_output = self.output_layer(decoder_output)\n","        return network_output\n","\n","    def forward(self, X):\n","        \"\"\"\n","        :param Tensor X: The input to run through the VAE. (N, input_shape[1], H, W)\n","\n","        :return:         A tuple consisting of the output of the Network,\n","                         and the mean-standard deviation layers.\n","                         (N, input_shape[1], H, W), (N, z_dim), (N, z_dim)\n","\n","        This method performs Forward Propagation through all the layers of the VAE and returns\n","        the reconstructed input.\n","        \"\"\"\n","        # encode the input to get mean and standard deviation\n","        mean, std = self._encode(X)\n","\n","        # get the latent vector z by using the reparameterization trick\n","        z = self._compute_latent_vector(mean, std)\n","\n","        # compute the output by propagating the latent vector through the decoder and return\n","        decoded_output = self._decode(z)\n","        return decoded_output, mean, std\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.scheduler_step_size, gamma=0.1)\n","        return [optimizer], [scheduler]\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"\n","        :param Tensor batch:  The current batch of the training set\n","        :param int batch_idx: The batch index of the current batch\n","\n","        :return:              A dictionary of the losses computed on the current prediction\n","        \"\"\"\n","        # unpack the current batch\n","        X, y = batch\n","\n","        # pass it through the model\n","        X_hat, mean, std = self(X)\n","        # calculate the losses\n","        losses = self.criterion(X, X_hat, mean, std)\n","\n","        return losses\n","\n","    def train_dataloader(self):\n","        \"\"\"\n","        :return:    A DataLoader object of the training set\n","        \"\"\"\n","        # download the training set using torchvision\n","        # if the set already exists in the provided path, it is not downloaded\n","        train_set = self.dataset_method(root=self.dataset_path, train=True, download=True,\n","                                        transform=torchvision.transforms.ToTensor())\n","        # initialize a pytorch DataLoader to feed training batches into the model\n","        self.train_loader = DataLoader(dataset=train_set, batch_size=self.batch_size, shuffle=True, num_workers=multiprocessing.cpu_count()//2)\n","        return self.train_loader\n","\n","    def test_step(self, batch, batch_idx):\n","        \"\"\"\n","        :param Tensor batch:  The current batch of the test set\n","        :param int batch_idx: The batch index of the current batch\n","\n","        :return:              A tuple consisting of a dictionary of the losses\n","                              computed on the current prediction and the MSE Loss\n","                              compared to the original picture\n","        \"\"\"\n","        # unpack the current batch\n","        X, y = batch\n","\n","        # pass it through the model\n","        X_hat, mean, std = self(X)\n","        # calculate the losses\n","        losses = self.criterion(X, X_hat, mean, std)\n","\n","        # also calculate the MSE loss\n","        mse_loss_func = torch.nn.MSELoss()\n","        mse_loss = mse_loss_func(X, X_hat)\n","\n","        self.log('mse_loss', mse_loss.item())\n","        self.log('losses', losses)\n","        return losses, mse_loss\n","\n","    def test_dataloader(self):\n","        \"\"\"\n","        :return:    A DataLoader object of the test set\n","        \"\"\"\n","        # download the test set using torchvision\n","        # if the set already exists in the provided path, it is not downloaded\n","        test_set = self.dataset_method(root=self.dataset_path, train=False, download=True,\n","                                        transform=torchvision.transforms.ToTensor())\n","        # initialize a pytorch DataLoader to feed test batches into the model\n","        self.test_loader = DataLoader(dataset=test_set, batch_size=self.batch_size, shuffle=True, num_workers=multiprocessing.cpu_count()//2)\n","        return self.test_loader\n","\n","    def sample(self, number_of_images):\n","        \"\"\"\n","        :param int number_of_images: The amount of images to compare against each other\n","\n","        :return:                     Does not return anything\n","\n","        This method plots the original images of the test set against the ones that were\n","        produced from the current model.\n","        \"\"\"\n","        # get one batch from the training set and unpack it\n","        X, y = next(iter(self.test_loader))\n","\n","        # pass it through the model\n","        X_hat, mean, std = self(X)\n","\n","        min_imgs = min(number_of_images, len(X))\n","        # for each image out of min_imgs\n","        for i in range(min_imgs):\n","            # if the image is rgb\n","            if (self.dataset_shape[0] == 3):\n","                # flatten the image\n","                X_np = X[i].detach().numpy().ravel()\n","                X_hat_np = X_hat[i].detach().numpy().ravel()\n","                # reshape it into (Width, Height, 3)\n","                X_np = np.reshape(X_np, (self.dataset_shape[1], self.dataset_shape[2], 3), order='F')\n","                X_hat_np = np.reshape(X_hat_np, (self.dataset_shape[1], self.dataset_shape[2], 3), order='F')\n","                # rotate it 270 degrees\n","                X_np = np.rot90(X_np, 3)\n","                X_hat_np = np.rot90(X_hat_np, 3)\n","\n","                # plot it against the original image\n","                plot_against(X_np, X_hat_np, y[i].item(), 'viridis')\n","            # if the image is grayscale\n","            elif (self.dataset_shape[0] == 1):\n","                # simply plot it against the original image\n","                plot_against(X[i][0].detach().numpy(), output[i][0].detach().numpy(), y[i].item(), 'gray')\n","\n","    @staticmethod\n","    def _data_fidelity_loss(X, X_hat, eps=1e-10):\n","        \"\"\"\n","        :param Tensor X:      The original input data that was passed to the VAE.\n","                               (N, input_shape[1], H, W)\n","        :param Tensor X_hat:  The reconstructed data, the output of the VAE.\n","                               (N, input_shape[1], H, W)\n","        :param Double eps:    A small positive double used to ensure we don't get log of 0.\n","\n","        :return:              A tensor containing the Data Fidelity term of the loss function,\n","                              which is given by the formula\n","\n","        E_{z ~ Q_{phi}(z | x)}[log(P_{theta}(x|z))] = sum(x * log(x_hat) + (1 - x) * log(1 - x_hat))\n","\n","            which is basically a Cross Entropy Loss.\n","\n","        This method computes the Data Fidelity term of the loss function. A small positive double\n","        epsilon is added inside the logarithm to make sure that we don't get log(0).\n","        \"\"\"\n","        # compute the data fidelity for every training example\n","        data_fidelity = torch.sum(X * torch.log(eps + X_hat) + (1 - X) * torch.log(eps + 1 - X_hat),\n","                                  axis=[1, 2, 3])\n","        return data_fidelity\n","\n","    @staticmethod\n","    def _kl_divergence_loss(mean, std):\n","        \"\"\"\n","        :param Tensor mean:   The output of the mean layer, computed with the output of the\n","                               encoder. (N, z_dim)\n","        :param Tensor std:    The output of the standard deviation layer, computed with the output\n","                               of the encoder. (N, z_dim)\n","\n","        :return:              A tensor consisting of the KL-Divergence term of the loss function,\n","                              which is given by the formula\n","\n","        D_{KL}[Q_{phi}(z | x) || P_{theta}(x)] = (1/2) * sum(std + mean^2 - 1 - log(std))\n","\n","            In the above equation we substitute std with e^{std} to improve numerical stability.\n","\n","        This method computes the KL-Divergence term of the loss function. It substitutes the\n","        value of the standard deviation layer with exp(standard deviation) in order to ensure\n","        numerical stability.\n","        \"\"\"\n","        # compute the kl divergence for each training example and return it\n","        kl_divergence = (1 / 2) * torch.sum(torch.exp(std) + torch.square(mean) - 1 - std, axis=1)\n","        return kl_divergence\n","\n","    def criterion(self, X, X_hat, mean, std):\n","        \"\"\"\n","        :param Tensor X:      The original input data that was passed to the VAE.\n","                               (N, input_shape[1], H, W)\n","        :param Tensor X_hat:  The reconstructed data, the output of the VAE.\n","                               (N, input_shape[1], H, W)\n","        :param Tensor mean:   The output of the mean layer, computed with the output of the\n","                               encoder. (N, z_dim)\n","        :param Tensor std:    The output of the standard deviation layer, computed with the output\n","                               of the encoder. (N, z_dim)\n","\n","        :return:              A dictionary containing the values of the losses computed.\n","\n","        This method computes the loss of the VAE using the formula:\n","\n","            L(x, x_hat) = - E_{z ~ Q_{phi}(z | x)}[log(P_{theta}(x|z))]\n","                          + D_{KL}[Q_{phi}(z | x) || P_{theta}(x)]\n","\n","        Intuitively, the expectation term is the Data Fidelity term, and the second term is a\n","        regularizer that makes sure the distribution of the encoder and the decoder stay close.\n","        \"\"\"\n","        # get the 2 losses\n","        data_fidelity_loss = VAE._data_fidelity_loss(X, X_hat)\n","        kl_divergence_loss = VAE._kl_divergence_loss(mean, std)\n","\n","        # add them, and then compute the mean over all training examples\n","        loss = -data_fidelity_loss + kl_divergence_loss\n","        loss = torch.mean(loss)\n","\n","        # place them all inside a dictionary and return it\n","        losses = {\"data_fidelity\": torch.mean(data_fidelity_loss),\n","                  \"kl-divergence\": torch.mean(kl_divergence_loss),\n","                  \"loss\": loss}\n","        return losses"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3YbhtuMXPTG"},"source":["###B-VAE"]},{"cell_type":"code","metadata":{"id":"mzH1LDJ_XRfU","executionInfo":{"status":"ok","timestamp":1607108231992,"user_tz":-120,"elapsed":9062,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["class betaVAE(VAE):\n","    \"\"\"\n","    Class that implements a Disentangled Variational Autoencoder (Beta VAE).\n","    This class inherits from the VAE class.\n","    \"\"\"\n","\n","    def __init__(self, architecture, hyperparameters, dataset_info):\n","        \"\"\"\n","        :param dict architecture:      A dictionary containing the hyperparameters that define the\n","                                       architecture of the model.\n","        :param dict hyperparameters:   A tuple that corresponds to the shape of the input.\n","        :param dict dataset_info:      The dimension of the latent vector z (bottleneck).\n","\n","        :return:                       An instance of the VAE class\n","\n","        The constructor of the Disentangled Variational Autoencoder.\n","        \"\"\"\n","\n","        # invoke the constructor of the VAE class, as the architecture is the same\n","        super(betaVAE, self).__init__(architecture, hyperparameters, dataset_info)\n","\n","        # store the value of beta in the class as it exists only in this VAE variation\n","        self.beta = hyperparameters[\"beta\"]\n","\n","\n","    def criterion(self, X, X_hat, mean, std):\n","        \"\"\"\n","        :param Tensor X:      The original input data that was passed to the B-VAE.\n","                               (N, input_shape[1], H, W)\n","        :param Tensor X_hat:  The reconstructed data, the output of the B-VAE.\n","                               (N, input_shape[1], H, W)\n","        :param Tensor mean:   The output of the mean layer, computed with the output of the\n","                               encoder. (N, z_dim)\n","        :param Tensor std:    The output of the standard deviation layer, computed with the output\n","                               of the encoder. (N, z_dim)\n","\n","        :return:              A dictionary containing the values of the losses computed.\n","\n","        This method computes the loss of the B-VAE using the formula:\n","\n","            L(x, x_hat) = - E_{z ~ Q_{phi}(z | x)}[log(P_{theta}(x|z))]\n","                          + beta * D_{KL}[Q_{phi}(z | x) || P_{theta}(x)]\n","\n","        Intuitively, the expectation term is the Data Fidelity term, and the second term is a\n","        regularizer that makes sure the distribution of the encoder and the decoder stay close.\n","        \"\"\"\n","        # get the 2 losses\n","        data_fidelity_loss = VAE._data_fidelity_loss(X, X_hat)\n","        kl_divergence_loss = VAE._kl_divergence_loss(mean, std)\n","\n","        # add them, and then compute the mean over all training examples\n","        loss = -data_fidelity_loss + self.beta * kl_divergence_loss\n","        loss = torch.mean(loss)\n","\n","        # place them all inside a dictionary and return it\n","        losses = {\"data_fidelity\": torch.mean(data_fidelity_loss),\n","                  \"kl-divergence\": torch.mean(kl_divergence_loss),\n","                  \"beta_kl-divergence\": torch.mean(self.beta * kl_divergence_loss),\n","                  \"loss\": loss}\n","        return losses"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H--U2gfVXdbc"},"source":["#Model Training"]},{"cell_type":"markdown","metadata":{"id":"4j38U8JZXxTY"},"source":["###Trainer Initialization"]},{"cell_type":"code","metadata":{"id":"mUfz6Xu2iJzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607108233554,"user_tz":-120,"elapsed":10618,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}},"outputId":"d7ca5e71-68cc-462f-854d-873a9eed08e8"},"source":["from pytorch_lightning import Trainer\n","\n","model = None\n","if (variation == \"VAE\"):\n","  model = VAE(architecture, hyperparameters, dataset_info)\n","elif (variation == \"B-VAE\"):\n","  model = betaVae(architecture, hyperparameters, dataset_info)\n","\n","trainer = Trainer(max_epochs = hyperparameters[\"epochs\"], gpus=1, fast_dev_run=False, progress_bar_refresh_rate=20)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"mVXcs2NdX59G"},"source":["###Model fitting (training)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253,"referenced_widgets":["1b31160c33c440f1831806645e9495ca","c52d6458466348a68bfb83af4b7b8361","2822a27fb2444a6fb12abfb1a338b4f9","6f168c746ccc41d48cdba0f24b5e2908","72a82e48b1f448279fba4e145d44aa9d","f6192dbffcd947d38e3e08d7ec18932a","8bb8159bab374924b30cebe36072b74b","e9109624cd55444da0b8fb57173ec8a7"]},"id":"zQXiOhf6X9f-","executionInfo":{"status":"ok","timestamp":1607109788059,"user_tz":-120,"elapsed":1565113,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}},"outputId":"a5047aef-7c8b-4e14-d38d-0d4d624dce74"},"source":["trainer.fit(model)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n","  | Name          | Type       | Params\n","---------------------------------------------\n","0 | encoder       | Sequential | 2.1 M \n","1 | mean_layer    | Linear     | 52.4 M\n","2 | std_layer     | Linear     | 52.4 M\n","3 | decoder_input | Linear     | 52.6 M\n","4 | decoder       | Sequential | 10.4 M\n","5 | output_layer  | Sequential | 347 K \n"],"name":"stderr"},{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b31160c33c440f1831806645e9495ca","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"2IVk_nbjYAxB"},"source":["#Model testing"]},{"cell_type":"markdown","metadata":{"id":"te7gGiJTYGwH"},"source":["###Testing on the test set"]},{"cell_type":"code","metadata":{"id":"51AenrmDid1i","colab":{"base_uri":"https://localhost:8080/","height":236,"referenced_widgets":["e32f32f3b8bf496889bab0f0fb087d21","5146752425ac46158bc3d467e85ad2cd","66e666f8c7de40d8bfcdec9b14a81337","a6c5c9b12937462880ae71927c1dc701","e7c61aee2f404a548f89755ff505228d","c3684f7e65f34974b31aff40176f9fb6","765bb6e9f9f4465fa750156128871652","dc27a6489b8b49d58581893441bff3d6"]},"executionInfo":{"status":"ok","timestamp":1607109801097,"user_tz":-120,"elapsed":1578139,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}},"outputId":"041bf93a-b78e-4a21-de78-cea0543c7b70"},"source":["result = trainer.test(model)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e32f32f3b8bf496889bab0f0fb087d21","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'losses': {'data_fidelity': tensor(-1905.3932, device='cuda:0'),\n","            'kl-divergence': tensor(48.3712, device='cuda:0'),\n","            'loss': tensor(1953.7644, device='cuda:0')},\n"," 'mse_loss': tensor(0.0296)}\n","--------------------------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gVRCAAxGYI39"},"source":["###Utility function to plot two images against each other"]},{"cell_type":"code","metadata":{"id":"fxdiiG8UYFpH","executionInfo":{"status":"ok","timestamp":1607109801097,"user_tz":-120,"elapsed":1578131,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}}},"source":["def plot_against(image1, image2, label, cmap):\n","    \"\"\"\n","    :param np.array image1:     An image stored as a numpy array\n","    :param np.array image2:     An image stored as a numpy array\n","    :param int label:           The label that corresponds to the images\n","    :param str cmap:            The colourmap to be used by pyplot\n","\n","    :return:                    Nothing\n","\n","    Function used to plot 2 images \"against\" each other using pyplot\n","    \"\"\"\n","    fig=plt.figure(figsize=(6, 6))\n","    title = \"Label {}\".format(label)\n","    fig.suptitle(title, fontsize=12)\n","    columns = 2\n","    rows = 1\n","\n","    im1 = fig.add_subplot(rows, columns, 1)\n","    title1 = \"Original\"\n","    im1.title.set_text(title1)\n","    plt.imshow(image1, cmap=cmap)\n","\n","    im2 = fig.add_subplot(rows, columns, 2)\n","    title2 = \"Generated\"\n","    im2.title.set_text(title2)\n","    plt.imshow(image2, cmap=cmap)\n","\n","\n","    plt.show()"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KDqPIPc4YQmn"},"source":["###Small sample of the model to evaluate the training"]},{"cell_type":"code","metadata":{"id":"xL6O3F-fihFd","colab":{"base_uri":"https://localhost:8080/","height":643},"executionInfo":{"status":"ok","timestamp":1607109807932,"user_tz":-120,"elapsed":1584961,"user":{"displayName":"Dr. Oblivion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNy6FhTlbRCfrWeGU2yVaSOJf5PitQmnlO9MwG=s64","userId":"02766168322280305280"}},"outputId":"d667cef9-4970-4725-d24e-3cb3090b74a2"},"source":["model = model.cpu()\n","model.sample(2)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x432 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x432 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}