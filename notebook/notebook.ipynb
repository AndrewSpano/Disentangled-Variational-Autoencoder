{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4fc74f846ef049df9ddff2dd7a3f6f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f4cd0d32252b43ff8f4fe94d939ddae8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_930f5a5b5ee3477592c0dcb95b3e6f13",
              "IPY_MODEL_5a5a79957f9c4d83a33297f9b0d28bcb"
            ]
          }
        },
        "f4cd0d32252b43ff8f4fe94d939ddae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "930f5a5b5ee3477592c0dcb95b3e6f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_444848c863ad45b887028f22c47d8788",
            "_dom_classes": [],
            "description": "Epoch 0: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff77b984517a4b3a838c9ede1559bd8f"
          }
        },
        "5a5a79957f9c4d83a33297f9b0d28bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19b7c59b7c92413d899e38b59d72e35f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  2.68it/s, loss=613.781, v_num=2]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dad3171afc2245cf962d0b703d2fd3aa"
          }
        },
        "444848c863ad45b887028f22c47d8788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff77b984517a4b3a838c9ede1559bd8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19b7c59b7c92413d899e38b59d72e35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dad3171afc2245cf962d0b703d2fd3aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f2af13987814ab5a04edbffb9518c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7dc432cbeff348cfa2bf42a66882e927",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99ce36ab5eca4edaafe8c7085a6bdf9c",
              "IPY_MODEL_12384715777341c3a9ecbd43027e1e52"
            ]
          }
        },
        "7dc432cbeff348cfa2bf42a66882e927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "99ce36ab5eca4edaafe8c7085a6bdf9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3aa86251cdb04f6cbbe42d2e538b072c",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_868b585cb9bc434d9921341900e97134"
          }
        },
        "12384715777341c3a9ecbd43027e1e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63210e78aebf4c17a7cdbda9885f3a13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  4.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81d644fa773647ecbb8bc1d19e44e2ab"
          }
        },
        "3aa86251cdb04f6cbbe42d2e538b072c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "868b585cb9bc434d9921341900e97134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63210e78aebf4c17a7cdbda9885f3a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81d644fa773647ecbb8bc1d19e44e2ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH8CShRYPYhv"
      },
      "source": [
        "#Installations & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBLVaAuonBIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bc9bd5-9339-4950-9cdf-0c936ab25f6e"
      },
      "source": [
        "!pip install pytorch-lightning"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: fsspec>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.8.4)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (0.8)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.33.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.35.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf1HdB31O5oq"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMBJm1iWV2m9"
      },
      "source": [
        "#Parameter Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq6avijbmPYw"
      },
      "source": [
        "import os\n",
        "# Insert your own path here\n",
        "ds_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Machine Learning\", \"Datasets\")\n",
        "\n",
        "variation = \"VAE\"\n",
        "\n",
        "configuration = {\n",
        "    \"dataset\": \"MNIST\",\n",
        "    \"path\": ds_path\n",
        "}\n",
        "\n",
        "architecture = {\n",
        "    \"conv_layers\": 3,\n",
        "    \"conv_channels\": [16, 32, 64],\n",
        "    \"conv_kernel_sizes\": [(7, 7), (7, 7), (7, 7)],\n",
        "    \"conv_strides\": [(1, 1), (1, 1), (1, 1)],\n",
        "    \"conv_paddings\": [(1, 1), (1, 1), (1, 1)],\n",
        "    \"z_dimension\": 64\n",
        "}\n",
        "\n",
        "hyperparameters = {\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 3e-7\n",
        "}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_6ejOn_V-YN"
      },
      "source": [
        "def prepare_dataset(configuration):\n",
        "    \"\"\"\n",
        "    :param dict configuration: The configuration dictionary returned by parse_config_file\n",
        "\n",
        "    :return:        A dictionary containing information about the dataset used\n",
        "\n",
        "    Function used to set some values used by the model based on the dataset selected\n",
        "    \"\"\"\n",
        "    dataset_info = {}\n",
        "    if (configuration[\"dataset\"] == \"MNIST\"):\n",
        "        dataset_info[\"ds_method\"] = torchvision.datasets.MNIST\n",
        "        dataset_info[\"ds_shape\"] = (1, 28, 28)\n",
        "        dataset_info[\"ds_path\"] = configuration[\"path\"]\n",
        "    elif (configuration[\"dataset\"] == \"CIFAR10\"):\n",
        "        dataset_info[\"ds_method\"] = torchvision.datasets.CIFAR10\n",
        "        dataset_info[\"ds_shape\"] = (3, 32, 32)\n",
        "        dataset_info[\"ds_path\"] = configuration[\"path\"]\n",
        "    else:\n",
        "        print(\"Currently only MNIST & CIFAR10 datasets are supported\")\n",
        "        return None\n",
        "\n",
        "    return dataset_info"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-eW2P6roRBo"
      },
      "source": [
        "dataset_info = prepare_dataset(configuration)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wcGIiNyWGre"
      },
      "source": [
        "#Model Declaration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8n44hIDWRcm"
      },
      "source": [
        "#####Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlfcNIqBWTW4"
      },
      "source": [
        "from torch import nn\n",
        "def compute_output_shape(current_shape, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    :param tuple current_shape:  The current shape of the data before a convolution is applied.\n",
        "    :param tuple kernel_size:    The kernel size of the current convolution operation.\n",
        "    :param tuple stride:         The stride of the current convolution operation.\n",
        "    :param tuple padding:        The padding of the current convolution operation.\n",
        "\n",
        "    :return:         The shape after a convolution operation with the above parameters is applied\n",
        "                     (as a tuple).\n",
        "                     The formula used to compute the final shape is\n",
        "\n",
        "        component[i] = floor((W[i] - K[i] + 2 * P[i]) / S[i]) + 1\n",
        "\n",
        "        where, W = input shape of the data\n",
        "               K = kernel size\n",
        "               P = padding\n",
        "               S = stride\n",
        "    \"\"\"\n",
        "    # get the dimension of the data\n",
        "    dimensions = len(current_shape)\n",
        "    # compute each component using the above formula and return\n",
        "    return tuple((current_shape[i] - kernel_size[i] + 2 * padding[i]) // stride[i] + 1\n",
        "                 for i in range(dimensions))\n",
        "\n",
        "\n",
        "def invalid_shape(current_shape):\n",
        "    \"\"\"\n",
        "    :param tuple current_shape: The current shape of the data after a convolution is applied.\n",
        "\n",
        "    :return:        True if the shape is invalid, that is, a negative or 0 components exists. Else,\n",
        "                    it returns False.\n",
        "    \"\"\"\n",
        "    # check all components\n",
        "    for component in current_shape:\n",
        "        if component <= 0:\n",
        "            return True\n",
        "    # return False if they are ok\n",
        "    return False\n",
        "\n",
        "\n",
        "def create_encoder(architecture, input_shape):\n",
        "    \"\"\"\n",
        "    :param dict architecture:  A dictionary containing the hyperparameters that define the\n",
        "                               architecture of the model.\n",
        "    :param tuple input_shape:  A tuple that corresponds to the shape of the input.\n",
        "\n",
        "    :return:             A PyTorch Sequential model that represents the encoder part of a VAE.\n",
        "\n",
        "    This method builds the encoder part of a VAE and returns it. It is common for all types of VAE.\n",
        "    \"\"\"\n",
        "\n",
        "    # the number of channels that the input image has\n",
        "    in_channels = input_shape[0]\n",
        "\n",
        "    # keep track of the current Height and Width of the image\n",
        "    current_shape = (input_shape[1], input_shape[2])\n",
        "\n",
        "    # build the encoder part\n",
        "    sets_of_conv_selu_bn = []\n",
        "\n",
        "    # iterate through the lists that define the architecture of the encoder\n",
        "    for layer in range(architecture[\"conv_layers\"]):\n",
        "\n",
        "        # define the number of output channels (filters) for this layer\n",
        "        out_channels = architecture[\"conv_channels\"][layer]\n",
        "\n",
        "        # add a set of Convolutional - SeLU - Batch Normalization sequential layers\n",
        "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                         kernel_size=architecture[\"conv_kernel_sizes\"][layer],\n",
        "                         stride=architecture[\"conv_strides\"][layer],\n",
        "                         padding=architecture[\"conv_paddings\"][layer])\n",
        "        selu = nn.LeakyReLU()\n",
        "        batch_norm = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # define a sequential model with the above architecture append it to the list\n",
        "        sets_of_conv_selu_bn.append(nn.Sequential(conv, selu, batch_norm))\n",
        "\n",
        "        # compute the new shape of the image\n",
        "        current_shape = compute_output_shape(current_shape,\n",
        "                                             architecture[\"conv_kernel_sizes\"][layer],\n",
        "                                             stride=architecture[\"conv_strides\"][layer],\n",
        "                                             padding=architecture[\"conv_paddings\"][layer])\n",
        "\n",
        "        # make sure that the shape is valid, and if not, raise an error\n",
        "        if invalid_shape(current_shape):\n",
        "            print(\"Architecture is invalid, please modify it\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        # the output channels of the current layer becomes the input channels of the next layer\n",
        "        in_channels = out_channels\n",
        "\n",
        "    # create a Sequential model and return it (* asterisk is used to unpack the list)\n",
        "    return nn.Sequential(*sets_of_conv_selu_bn), current_shape\n",
        "\n",
        "\n",
        "def create_decoder(architecture):\n",
        "    \"\"\"\n",
        "    :param dict architecture:  A dictionary containing the hyperparameters that define the\n",
        "                               architecture of the model.\n",
        "\n",
        "    :return:            A PyTorch Sequential model that represents the decoder part of a VAE.\n",
        "\n",
        "    This method builds the decoder part of a VAE and returns it. It is common for all types of VAE.\n",
        "    \"\"\"\n",
        "    # now start building the decoder part\n",
        "    sets_of_convtr_selu_bn = []\n",
        "\n",
        "    # define the current number of channels (after the reformation of the latent vector z)\n",
        "    in_channels = architecture[\"conv_channels\"][-1]\n",
        "\n",
        "    # iterate through the lists that define the architecture of the decoder\n",
        "    for layer in range(architecture[\"conv_layers\"] - 1, -1, -1):\n",
        "\n",
        "        # define the number of output channels (filters) for this layer\n",
        "        out_channels = architecture[\"conv_channels\"][layer]\n",
        "\n",
        "        # add a set of ConvolutionalTranspose - SeLU - Batch Normalization sequential layers\n",
        "        convtr = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                    kernel_size=architecture[\"conv_kernel_sizes\"][layer],\n",
        "                                    stride=architecture[\"conv_strides\"][layer],\n",
        "                                    padding=architecture[\"conv_paddings\"][layer])\n",
        "        selu = nn.LeakyReLU()\n",
        "        batch_norm = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # define a sequential model with this architecture append it to the list\n",
        "        sets_of_convtr_selu_bn.append(nn.Sequential(convtr, selu, batch_norm))\n",
        "\n",
        "        # the output channels of the current layer becomes the input channels of the next layer\n",
        "        in_channels = out_channels\n",
        "\n",
        "    # create a Sequential model and return it (* asterisk is used to unpack the list)\n",
        "    return nn.Sequential(*sets_of_convtr_selu_bn)\n",
        "\n",
        "\n",
        "def create_output_layer(architecture, input_shape):\n",
        "    \"\"\"\n",
        "    :param dict architecture:  A dictionary containing the hyperparameters that define the\n",
        "                               architecture of the model.\n",
        "    :param tuple input_shape:  A tuple that corresponds to the shape of the input.\n",
        "\n",
        "    :return:             A PyTorch Sequential model that represents the output layer of a VAE.\n",
        "\n",
        "    This method creates the output layer of a VAE, that is, the layer where the data from the\n",
        "    output of the decoder gets fed in order to be finally reconstructed.\n",
        "    \"\"\"\n",
        "\n",
        "    # define the number of input channels of the last layer\n",
        "    in_channels = architecture[\"conv_channels\"][0]\n",
        "\n",
        "    # define the output layer: ConvTranspose2d -> SELU -> Batch Norm -> Conv2d -> Sigmoid\n",
        "    convtr = nn.ConvTranspose2d(in_channels=in_channels, out_channels=in_channels,\n",
        "                                kernel_size=architecture[\"conv_kernel_sizes\"][0],\n",
        "                                stride=architecture[\"conv_strides\"][0],\n",
        "                                padding=architecture[\"conv_paddings\"][0])\n",
        "    selu = nn.LeakyReLU()\n",
        "    batch_norm = nn.BatchNorm2d(in_channels)\n",
        "    conv = nn.Conv2d(in_channels=in_channels, out_channels=input_shape[0],\n",
        "                     kernel_size=architecture[\"conv_kernel_sizes\"][0],\n",
        "                     stride=architecture[\"conv_strides\"][0],\n",
        "                     padding=architecture[\"conv_paddings\"][0])\n",
        "    sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # create a Sequential model and return it\n",
        "    return nn.Sequential(convtr, selu, batch_norm, conv, sigmoid)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqv8srojWLdA"
      },
      "source": [
        "###Variational Auto-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJePIeJfWKqF"
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "class VAE(pl.LightningModule):\n",
        "    \"\"\" Class that implements a Variational Autoencoder \"\"\"\n",
        "\n",
        "    def __init__(self, architecture, hyperparameters, dataset_info):\n",
        "        \"\"\"\n",
        "        :param dict architecture:      A dictionary containing the hyperparameters that define the\n",
        "                                       architecture of the model.\n",
        "        :param dict hyperparameters:   A tuple that corresponds to the shape of the input.\n",
        "        :param dict dataset_info:      The dimension of the latent vector z (bottleneck).\n",
        "\n",
        "        :return:                       An instance of the VAE class\n",
        "\n",
        "        The constructor of the Variational Autoencoder.\n",
        "        \"\"\"\n",
        "\n",
        "        # call the constructor of the super class\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # initialize class variables regarding the architecture of the model\n",
        "        self.conv_layers = architecture[\"conv_layers\"]\n",
        "        self.conv_channels = architecture[\"conv_channels\"]\n",
        "        self.conv_kernel_sizes = architecture[\"conv_kernel_sizes\"]\n",
        "        self.conv_strides = architecture[\"conv_strides\"]\n",
        "        self.conv_paddings = architecture[\"conv_paddings\"]\n",
        "        self.z_dim = architecture[\"z_dimension\"]\n",
        "\n",
        "        # unpack the \"hyperparameters\" dictionary\n",
        "        self.batch_size = hyperparameters[\"batch_size\"]\n",
        "        self.learning_rate = hyperparameters[\"learning_rate\"]\n",
        "        self.scheduler_step_size = hyperparameters[\"epochs\"]//2\n",
        "\n",
        "        # unpack the \"dataset_info\" dictionary\n",
        "        self.dataset_method = dataset_info[\"ds_method\"]\n",
        "        self.dataset_shape = dataset_info[\"ds_shape\"]\n",
        "        self.dataset_path = dataset_info[\"ds_path\"]\n",
        "\n",
        "        # build the encoder\n",
        "        self.encoder, self.encoder_output_shape = create_encoder(architecture, self.dataset_shape)\n",
        "\n",
        "        # compute the length of the output of the decoder once it has been flattened\n",
        "        in_features = self.conv_channels[-1] * np.prod(self.encoder_output_shape[:])\n",
        "        # now define the mean and standard deviation layers\n",
        "        self.mean_layer = nn.Linear(in_features=in_features, out_features=self.z_dim)\n",
        "        self.std_layer = nn.Linear(in_features=in_features, out_features=self.z_dim)\n",
        "\n",
        "        # use a linear layer for the input of the decoder\n",
        "        in_channels = self.conv_channels[-1]\n",
        "        self.decoder_input = nn.Linear(in_features=self.z_dim, out_features=in_features)\n",
        "\n",
        "        # build the decoder\n",
        "        self.decoder = create_decoder(architecture)\n",
        "\n",
        "        # build the output layer\n",
        "        self.output_layer = create_output_layer(architecture, self.dataset_shape)\n",
        "\n",
        "    def _encode(self, X):\n",
        "        \"\"\"\n",
        "        :param Tensor X:  Input to encode into mean and standard deviation.\n",
        "                          (N, input_shape[1], H, W)\n",
        "\n",
        "        :return:          A tuple with the mean and std tensors that the encoder produces\n",
        "                          for input X. (N, z_dim)\n",
        "\n",
        "        This method applies forward propagation to the self.encoder in order to get the mean and\n",
        "        standard deviation of the latent vector z.\n",
        "        \"\"\"\n",
        "        # run the input through the encoder part of the Network\n",
        "        encoded_input = self.encoder(X)\n",
        "\n",
        "        # flatten so that it can be fed to the mean and standard deviation layers\n",
        "        encoded_input = torch.flatten(encoded_input, start_dim=1)\n",
        "\n",
        "        # compute the mean and standard deviation\n",
        "        mean = self.mean_layer(encoded_input)\n",
        "        std = self.std_layer(encoded_input)\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "    def _compute_latent_vector(self, mean, std):\n",
        "        \"\"\"\n",
        "        :param Tensor mean:  The mean of the latent vector z following a Gaussian distribution.\n",
        "                             (N, z_dim)\n",
        "        :param Tensor std:   The standard deviation of the latent vector z following a Gaussian\n",
        "                             distribution. (N, z_dim)\n",
        "\n",
        "        :return:             A Tensor of the Linear combination of the mean and standard deviation, where the latter\n",
        "                             factor is multiplied with a random variable epsilon ~ N(0, 1). Basically\n",
        "                             the latent vector z. (N, z_dim)\n",
        "\n",
        "        This method computes the latent vector z by applying the reparameterization trick to the\n",
        "        output of the mean and standard deviation layers, in order to be able to later compute the\n",
        "        gradient. The stochasticiy here is introduced by the factor epsilon, which is an independent\n",
        "        node. Thus, we do not have to compute its gradient during backpropagation.\n",
        "        \"\"\"\n",
        "\n",
        "        # compute the stochastic node epsilon\n",
        "        epsilon = torch.randn_like(std)\n",
        "        # raise the standard deviation to an exponent, to improve numberical stability\n",
        "        std = torch.exp(1/2 * std)\n",
        "\n",
        "        # compute the linear combination of the above attributes and return\n",
        "        return mean + epsilon * std\n",
        "\n",
        "    def _decode(self, z):\n",
        "        \"\"\"\n",
        "        :param Tensor z:   Latent vector computed using the mean and variance layers (with the\n",
        "                           reparameterization trick). (N, z_dim)\n",
        "\n",
        "        :return:           A Tensor with The output of the decoder part of the network.\n",
        "                           (N, input_shape[1], H, W)\n",
        "\n",
        "        This method performs forward propagation of the latent vector through the decoder of the\n",
        "        VAE to get the final output of the network.\n",
        "        \"\"\"\n",
        "        # run the latent vector through the \"input decoder\" layer\n",
        "        decoder_input = self.decoder_input(z)\n",
        "\n",
        "        # convert back the shape that will be fed to the decoder\n",
        "        height = self.encoder_output_shape[0]\n",
        "        width = self.encoder_output_shape[1]\n",
        "        decoder_input = decoder_input.view(-1, self.conv_channels[-1], height, width)\n",
        "\n",
        "        # run through the decoder\n",
        "        decoder_output = self.decoder(decoder_input)\n",
        "\n",
        "        # run through the output layer and return\n",
        "        network_output = self.output_layer(decoder_output)\n",
        "        return network_output\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        :param Tensor X: The input to run through the VAE. (N, input_shape[1], H, W)\n",
        "\n",
        "        :return:         A tuple consisting of the output of the Network,\n",
        "                         and the mean-standard deviation layers.\n",
        "                         (N, input_shape[1], H, W), (N, z_dim), (N, z_dim)\n",
        "\n",
        "        This method performs Forward Propagation through all the layers of the VAE and returns\n",
        "        the reconstructed input.\n",
        "        \"\"\"\n",
        "        # encode the input to get mean and standard deviation\n",
        "        mean, std = self._encode(X)\n",
        "\n",
        "        # get the latent vector z by using the reparameterization trick\n",
        "        z = self._compute_latent_vector(mean, std)\n",
        "\n",
        "        # compute the output by propagating the latent vector through the decoder and return\n",
        "        decoded_output = self._decode(z)\n",
        "        return decoded_output, mean, std\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.scheduler_step_size, gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        :param Tensor batch:  The current batch of the training set\n",
        "        :param int batch_idx: The batch index of the current batch\n",
        "\n",
        "        :return:              A dictionary of the losses computed on the current prediction\n",
        "        \"\"\"\n",
        "        # unpack the current batch\n",
        "        X, y = batch\n",
        "\n",
        "        # pass it through the model\n",
        "        X_hat, mean, std = self(X)\n",
        "        # calculate the losses\n",
        "        losses = self.criterion(X, X_hat, mean, std)\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"\n",
        "        :return:    A DataLoader object of the training set\n",
        "        \"\"\"\n",
        "        # download the training set using torchvision\n",
        "        # if the set already exists in the provided path, it is not downloaded\n",
        "        train_set = self.dataset_method(root=self.dataset_path, train=True, download=True,\n",
        "                                        transform=torchvision.transforms.ToTensor())\n",
        "        # initialize a pytorch DataLoader to feed training batches into the model\n",
        "        self.train_loader = DataLoader(dataset=train_set, batch_size=self.batch_size, shuffle=True, num_workers=multiprocessing.cpu_count()//2)\n",
        "        return self.train_loader\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        :param Tensor batch:  The current batch of the test set\n",
        "        :param int batch_idx: The batch index of the current batch\n",
        "\n",
        "        :return:              A tuple consisting of a dictionary of the losses\n",
        "                              computed on the current prediction and the MSE Loss\n",
        "                              compared to the original picture\n",
        "        \"\"\"\n",
        "        # unpack the current batch\n",
        "        X, y = batch\n",
        "\n",
        "        # pass it through the model\n",
        "        X_hat, mean, std = self(X)\n",
        "        # calculate the losses\n",
        "        losses = self.criterion(X, X_hat, mean, std)\n",
        "\n",
        "        # also calculate the MSE loss\n",
        "        mse_loss_func = torch.nn.MSELoss()\n",
        "        mse_loss = mse_loss_func(X, X_hat)\n",
        "\n",
        "        self.log('mse_loss', mse_loss.item())\n",
        "        self.log('losses', losses)\n",
        "        return losses, mse_loss\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"\n",
        "        :return:    A DataLoader object of the test set\n",
        "        \"\"\"\n",
        "        # download the test set using torchvision\n",
        "        # if the set already exists in the provided path, it is not downloaded\n",
        "        test_set = self.dataset_method(root=self.dataset_path, train=False, download=True,\n",
        "                                        transform=torchvision.transforms.ToTensor())\n",
        "        # initialize a pytorch DataLoader to feed test batches into the model\n",
        "        self.test_loader = DataLoader(dataset=test_set, batch_size=self.batch_size, shuffle=True, num_workers=multiprocessing.cpu_count()//2)\n",
        "        return self.test_loader\n",
        "\n",
        "    def sample(self, number_of_images):\n",
        "        \"\"\"\n",
        "        :param int number_of_images: The amount of images to compare against each other\n",
        "\n",
        "        :return:                     Does not return anything\n",
        "\n",
        "        This method plots n generated images next to each other\n",
        "        \"\"\"\n",
        "\n",
        "        # get as many batches from the test set to fill the final plot\n",
        "        tensors = []\n",
        "        img_count = 0\n",
        "        while number_of_images*number_of_images > img_count:\n",
        "            batch, y = next(iter(self.test_loader))\n",
        "            img_count += len(batch)\n",
        "            tensors.append(batch)\n",
        "\n",
        "        # concatenate them\n",
        "        X = torch.cat(tensors, dim=0)\n",
        "\n",
        "        # pass them through the model\n",
        "        X_hat, mean, std = self(X)\n",
        "\n",
        "        min_imgs = min(number_of_images, len(X))\n",
        "\n",
        "        # set the correct colourmap that corresponds to the image dimension\n",
        "        cmap = None\n",
        "        if (self.dataset_shape[0] == 3):\n",
        "            cmap = 'viridis'\n",
        "        elif (self.dataset_shape[0] == 1):\n",
        "            cmap = 'gray'\n",
        "\n",
        "        plot_multiple(X_hat.detach().numpy(), min_imgs, self.dataset_shape, cmap)\n",
        "\n",
        "    @staticmethod\n",
        "    def _data_fidelity_loss(X, X_hat, eps=1e-10):\n",
        "        \"\"\"\n",
        "        :param Tensor X:      The original input data that was passed to the VAE.\n",
        "                               (N, input_shape[1], H, W)\n",
        "        :param Tensor X_hat:  The reconstructed data, the output of the VAE.\n",
        "                               (N, input_shape[1], H, W)\n",
        "        :param Double eps:    A small positive double used to ensure we don't get log of 0.\n",
        "\n",
        "        :return:              A tensor containing the Data Fidelity term of the loss function,\n",
        "                              which is given by the formula\n",
        "\n",
        "        E_{z ~ Q_{phi}(z | x)}[log(P_{theta}(x|z))] = sum(x * log(x_hat) + (1 - x) * log(1 - x_hat))\n",
        "\n",
        "            which is basically a Cross Entropy Loss.\n",
        "\n",
        "        This method computes the Data Fidelity term of the loss function. A small positive double\n",
        "        epsilon is added inside the logarithm to make sure that we don't get log(0).\n",
        "        \"\"\"\n",
        "        # compute the data fidelity for every training example\n",
        "        data_fidelity = torch.sum(X * torch.log(eps + X_hat) + (1 - X) * torch.log(eps + 1 - X_hat),\n",
        "                                  axis=[1, 2, 3])\n",
        "        return data_fidelity\n",
        "\n",
        "    @staticmethod\n",
        "    def _kl_divergence_loss(mean, std):\n",
        "        \"\"\"\n",
        "        :param Tensor mean:   The output of the mean layer, computed with the output of the\n",
        "                               encoder. (N, z_dim)\n",
        "        :param Tensor std:    The output of the standard deviation layer, computed with the output\n",
        "                               of the encoder. (N, z_dim)\n",
        "\n",
        "        :return:              A tensor consisting of the KL-Divergence term of the loss function,\n",
        "                              which is given by the formula\n",
        "\n",
        "        D_{KL}[Q_{phi}(z | x) || P_{theta}(x)] = (1/2) * sum(std + mean^2 - 1 - log(std))\n",
        "\n",
        "            In the above equation we substitute std with e^{std} to improve numerical stability.\n",
        "\n",
        "        This method computes the KL-Divergence term of the loss function. It substitutes the\n",
        "        value of the standard deviation layer with exp(standard deviation) in order to ensure\n",
        "        numerical stability.\n",
        "        \"\"\"\n",
        "        # compute the kl divergence for each training example and return it\n",
        "        kl_divergence = (1 / 2) * torch.sum(torch.exp(std) + torch.square(mean) - 1 - std, axis=1)\n",
        "        return kl_divergence\n",
        "\n",
        "    def criterion(self, X, X_hat, mean, std):\n",
        "        \"\"\"\n",
        "        :param Tensor X:      The original input data that was passed to the VAE.\n",
        "                               (N, input_shape[1], H, W)\n",
        "        :param Tensor X_hat:  The reconstructed data, the output of the VAE.\n",
        "                               (N, input_shape[1], H, W)\n",
        "        :param Tensor mean:   The output of the mean layer, computed with the output of the\n",
        "                               encoder. (N, z_dim)\n",
        "        :param Tensor std:    The output of the standard deviation layer, computed with the output\n",
        "                               of the encoder. (N, z_dim)\n",
        "\n",
        "        :return:              A dictionary containing the values of the losses computed.\n",
        "\n",
        "        This method computes the loss of the VAE using the formula:\n",
        "\n",
        "            L(x, x_hat) = - E_{z ~ Q_{phi}(z | x)}[log(P_{theta}(x|z))]\n",
        "                          + D_{KL}[Q_{phi}(z | x) || P_{theta}(x)]\n",
        "\n",
        "        Intuitively, the expectation term is the Data Fidelity term, and the second term is a\n",
        "        regularizer that makes sure the distribution of the encoder and the decoder stay close.\n",
        "        \"\"\"\n",
        "        # get the 2 losses\n",
        "        data_fidelity_loss = VAE._data_fidelity_loss(X, X_hat)\n",
        "        kl_divergence_loss = VAE._kl_divergence_loss(mean, std)\n",
        "\n",
        "        # add them, and then compute the mean over all training examples\n",
        "        loss = -data_fidelity_loss + kl_divergence_loss\n",
        "        loss = torch.mean(loss)\n",
        "\n",
        "        # place them all inside a dictionary and return it\n",
        "        losses = {\"data_fidelity\": torch.mean(data_fidelity_loss),\n",
        "                  \"kl-divergence\": torch.mean(kl_divergence_loss),\n",
        "                  \"loss\": loss}\n",
        "        return losses"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3YbhtuMXPTG"
      },
      "source": [
        "###B-VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzH1LDJ_XRfU"
      },
      "source": [
        "class betaVAE(VAE):\n",
        "    \"\"\"\n",
        "    Class that implements a Disentangled Variational Autoencoder (Beta VAE).\n",
        "    This class inherits from the VAE class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, architecture, hyperparameters, dataset_info):\n",
        "        \"\"\"\n",
        "        :param dict architecture:      A dictionary containing the hyperparameters that define the\n",
        "                                       architecture of the model.\n",
        "        :param dict hyperparameters:   A tuple that corresponds to the shape of the input.\n",
        "        :param dict dataset_info:      The dimension of the latent vector z (bottleneck).\n",
        "\n",
        "        :return:                       An instance of the VAE class\n",
        "\n",
        "        The constructor of the Disentangled Variational Autoencoder.\n",
        "        \"\"\"\n",
        "\n",
        "        # invoke the constructor of the VAE class, as the architecture is the same\n",
        "        super(betaVAE, self).__init__(architecture, hyperparameters, dataset_info)\n",
        "\n",
        "        # store the value of beta in the class as it exists only in this VAE variation\n",
        "        self.beta = hyperparameters[\"beta\"]\n",
        "\n",
        "\n",
        "    def criterion(self, X, X_hat, mean, std):\n",
        "        \"\"\"\n",
        "        :param Tensor X:      The original input data that was passed to the B-VAE.\n",
        "                               (N, input_shape[1], H, W)\n",
        "        :param Tensor X_hat:  The reconstructed data, the output of the B-VAE.\n",
        "                               (N, input_shape[1], H, W)\n",
        "        :param Tensor mean:   The output of the mean layer, computed with the output of the\n",
        "                               encoder. (N, z_dim)\n",
        "        :param Tensor std:    The output of the standard deviation layer, computed with the output\n",
        "                               of the encoder. (N, z_dim)\n",
        "\n",
        "        :return:              A dictionary containing the values of the losses computed.\n",
        "\n",
        "        This method computes the loss of the B-VAE using the formula:\n",
        "\n",
        "            L(x, x_hat) = - E_{z ~ Q_{phi}(z | x)}[log(P_{theta}(x|z))]\n",
        "                          + beta * D_{KL}[Q_{phi}(z | x) || P_{theta}(x)]\n",
        "\n",
        "        Intuitively, the expectation term is the Data Fidelity term, and the second term is a\n",
        "        regularizer that makes sure the distribution of the encoder and the decoder stay close.\n",
        "        \"\"\"\n",
        "        # get the 2 losses\n",
        "        data_fidelity_loss = VAE._data_fidelity_loss(X, X_hat)\n",
        "        kl_divergence_loss = VAE._kl_divergence_loss(mean, std)\n",
        "\n",
        "        # add them, and then compute the mean over all training examples\n",
        "        loss = -data_fidelity_loss + self.beta * kl_divergence_loss\n",
        "        loss = torch.mean(loss)\n",
        "\n",
        "        # place them all inside a dictionary and return it\n",
        "        losses = {\"data_fidelity\": torch.mean(data_fidelity_loss),\n",
        "                  \"kl-divergence\": torch.mean(kl_divergence_loss),\n",
        "                  \"beta_kl-divergence\": torch.mean(self.beta * kl_divergence_loss),\n",
        "                  \"loss\": loss}\n",
        "        return losses"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H--U2gfVXdbc"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j38U8JZXxTY"
      },
      "source": [
        "###Trainer Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfz6Xu2iJzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc27bc5-3936-416c-f878-421da75efd8f"
      },
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "model = None\n",
        "if (variation == \"VAE\"):\n",
        "  model = VAE(architecture, hyperparameters, dataset_info)\n",
        "elif (variation == \"B-VAE\"):\n",
        "  model = betaVae(architecture, hyperparameters, dataset_info)\n",
        "\n",
        "trainer = Trainer(max_epochs = hyperparameters[\"epochs\"], gpus=1, fast_dev_run=False, progress_bar_refresh_rate=20)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVXcs2NdX59G"
      },
      "source": [
        "###Model fitting (training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236,
          "referenced_widgets": [
            "4fc74f846ef049df9ddff2dd7a3f6f23",
            "f4cd0d32252b43ff8f4fe94d939ddae8",
            "930f5a5b5ee3477592c0dcb95b3e6f13",
            "5a5a79957f9c4d83a33297f9b0d28bcb",
            "444848c863ad45b887028f22c47d8788",
            "ff77b984517a4b3a838c9ede1559bd8f",
            "19b7c59b7c92413d899e38b59d72e35f",
            "dad3171afc2245cf962d0b703d2fd3aa"
          ]
        },
        "id": "zQXiOhf6X9f-",
        "outputId": "5e2d5afa-fcdc-47f8-9379-7d08b0c96922"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name          | Type       | Params\n",
            "---------------------------------------------\n",
            "0 | encoder       | Sequential | 126 K \n",
            "1 | mean_layer    | Linear     | 1.0 M \n",
            "2 | std_layer     | Linear     | 1.0 M \n",
            "3 | decoder_input | Linear     | 1.1 M \n",
            "4 | decoder       | Sequential | 326 K \n",
            "5 | output_layer  | Sequential | 13.4 K\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fc74f846ef049df9ddff2dd7a3f6f23",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IVk_nbjYAxB"
      },
      "source": [
        "#Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te7gGiJTYGwH"
      },
      "source": [
        "###Testing on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51AenrmDid1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239,
          "referenced_widgets": [
            "6f2af13987814ab5a04edbffb9518c5c",
            "7dc432cbeff348cfa2bf42a66882e927",
            "99ce36ab5eca4edaafe8c7085a6bdf9c",
            "12384715777341c3a9ecbd43027e1e52",
            "3aa86251cdb04f6cbbe42d2e538b072c",
            "868b585cb9bc434d9921341900e97134",
            "63210e78aebf4c17a7cdbda9885f3a13",
            "81d644fa773647ecbb8bc1d19e44e2ab"
          ]
        },
        "outputId": "028d9dcb-a537-49f2-f53b-a9e2911b0ecf"
      },
      "source": [
        "result = trainer.test(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f2af13987814ab5a04edbffb9518c5c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'losses': {'data_fidelity': tensor(-548.4006, device='cuda:0'),\n",
            "            'kl-divergence': tensor(0.0117, device='cuda:0'),\n",
            "            'loss': tensor(548.4124, device='cuda:0')},\n",
            " 'mse_loss': tensor(0.2350)}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVRCAAxGYI39"
      },
      "source": [
        "###Utility function to plot two images against each other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxdiiG8UYFpH"
      },
      "source": [
        "def plot_multiple(images, n, dim, cmap):\n",
        "    \"\"\"\n",
        "    :param arr images:          An array of images stored as a numpy array\n",
        "    :param int n:               The width and height of the plot in terms of images\n",
        "    :param tuple dim:           The dimension of the images\n",
        "    :param str cmap:            The colourmap to be used by pyplot\n",
        "\n",
        "    :return:                    Nothing\n",
        "\n",
        "    Function used to plot multiple images in one single plot\n",
        "    \"\"\"\n",
        "    # unpack the image dimensions\n",
        "    z_dim, x_dim, y_dim = dim\n",
        "\n",
        "    # if image is grayscale\n",
        "    if (z_dim == 1):\n",
        "        # initialize some limits on x&y\n",
        "        x_limit = np.linspace(-2, 2, n)\n",
        "        y_limit = np.linspace(-2, 2, n)\n",
        "\n",
        "        # initialize the final combined image\n",
        "        empty = np.empty((x_dim*n, y_dim*n))\n",
        "\n",
        "        current = 0\n",
        "        for i, zi in enumerate(x_limit):\n",
        "            for j, pi in enumerate(y_limit):\n",
        "                # each image insert it into a subsection of the final image\n",
        "                empty[(n-i-1)*x_dim:(n-i)*x_dim, j*y_dim:(j+1)*y_dim] = images[current][0]\n",
        "                current+=1\n",
        "\n",
        "        plt.figure(figsize=(8, 10))\n",
        "\n",
        "        x,y = np.meshgrid(x_limit, y_limit)\n",
        "        plt.imshow(empty, origin=\"upper\", cmap=cmap)\n",
        "        plt.grid(False)\n",
        "        plt.show()\n",
        "\n",
        "    # if the image is rgb\n",
        "    elif (z_dim == 3):\n",
        "        # initialize some limits on x&y\n",
        "        x_limit = np.linspace(-2, 2, n)\n",
        "        y_limit = np.linspace(-2, 2, n)\n",
        "\n",
        "        # initialize the final combined image (now with one more dim)\n",
        "        empty = np.empty((x_dim*n, y_dim*n, 3))\n",
        "\n",
        "        current = 0\n",
        "        for i, zi in enumerate(x_limit):\n",
        "            for j, pi in enumerate(y_limit):\n",
        "                # flatten the image\n",
        "                curr_img = images[current].ravel()\n",
        "                # reshape it into the correct shape for pyplot\n",
        "                curr_img = np.reshape(curr_img, (x_dim, y_dim, z_dim), order='F')\n",
        "                # rotate it by 270 degrees\n",
        "                curr_img = np.rot90(curr_img, 3)\n",
        "\n",
        "                # insert it into a subsection of the final image\n",
        "                empty[(n-i-1)*x_dim:(n-i)*x_dim, j*y_dim:(j+1)*y_dim] = curr_img\n",
        "                current+=1\n",
        "\n",
        "        plt.figure(figsize=(8, 10))\n",
        "\n",
        "        x,y = np.meshgrid(x_limit, y_limit)\n",
        "        plt.imshow(empty, origin=\"upper\", cmap=cmap)\n",
        "        plt.grid(False)\n",
        "        plt.show()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDqPIPc4YQmn"
      },
      "source": [
        "###Small sample of the model to evaluate the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL6O3F-fihFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "b3a460e4-1849-4675-f9d3-e9eac0d4606c"
      },
      "source": [
        "model = model.cpu()\n",
        "model.sample(6)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}